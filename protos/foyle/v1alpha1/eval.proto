syntax = "proto3";

import "foyle/v1alpha1/agent.proto";
import "foyle/v1alpha1/doc.proto";
import "foyle/v1alpha1/trainer.proto";
import "runme/parser/v1/parser.proto";
import "google/protobuf/timestamp.proto";
import "google/protobuf/struct.proto";

option go_package = "github.com/jlewi/foyle/protos/go/foyle/v1alpha1";

enum EvalResultStatus {
  UNKNOWN_EVAL_RESULT_STATUS = 0;
  DONE = 1;
  ERROR = 2;
}

enum AssertResult {
  UNKNOWN_AssertResult = 0;
  PASSED = 1;
  FAILED = 2;
  SKIPPED = 3;
}

enum CellsMatchResult {
  UNKNOWN_CellsMatchResult = 0;
  MATCH = 1;
  MISMATCH = 2;
}

// EvalResult represents an evaluation result
message EvalResult {
  // Example is the answer and expected result
  EvalExample example = 1;

  repeated runme.parser.v1.Cell actual_cells = 11;

  // Error indicates an error generating the completion.
  string error = 5;

  // Status of the evaluation
  EvalResultStatus status = 6;

  // The ID of the generate trace
  string gen_trace_id = 8;

  // Best matching RAG result
  RAGResult best_rag_result = 9;

  repeated Assertion assertions = 10;


  // cells_match_result is the LLM judge's evaluation of whether the actual and expected response match
  // We use an enum so we can encode unknown
  CellsMatchResult cells_match_result = 12;

  // Explanation given by the LLM judge
  string judge_explanation = 13;

  // Removed fields
  // example_file is the file containing the example
  // string example_file = 2;
  // Actual response
  // repeated Block actual = 3;
  // The distance between the actual and expected response
  // int32 distance = 4;
  // float normalized_distance = 7;
  reserved 2, 3, 4, 7;
}

message Assertion {
  // Name of the assertion
  string name = 1;
  AssertResult result = 2;
  // Human readable detail of the assertion. If there was an error this should contain the error message.
  string detail = 3;
}

message EvalResultListRequest {
  // The path of the database to fetch results for
  string database = 1;
}

message EvalResultListResponse {
  repeated EvalResult items = 1;
}


// AssertionRow represents a row in the assertion table.
// It is intended for returning the results of assertions. In a way that makes it easy to view the assertions
// in a table inside a RunMe notebook. So we need to flatten the data.
message AssertionRow {
  // id of the example
  string id = 1;

  string exampleFile = 2;

  // Document markdown
  string doc_md = 3;
  string answer_md =4;

  // TODO(jeremy): How can we avoid having to add each assertion here
  AssertResult code_after_markdown = 5;
  AssertResult one_code_cell = 6;
  AssertResult ends_with_code_cell = 7;
}

message AssertionTableRequest {
  // The path of the database to fetch results for
  string database = 1;
}

// EvalExample is a datapoint for evaluation
message EvalExample {
  // TODO(jeremy): Right now we are using the id to encode the sessionId that the eval example is associated with.
  // Should we add a sessionId field and not make them the same?
  string id = 1;

  // time is the time corresponding to the example.
  // Examples need to be replayed in the same order they actually occurred to avoid cheatin
  google.protobuf.Timestamp time  = 4;

  // FullContext is the context used as input
  FullContext full_context = 2;

  // Expected cells is the expected value for generation
  repeated runme.parser.v1.Cell expected_cells = 3;
}


message AssertionTableResponse {
  repeated AssertionRow rows = 1;
}

service EvalService {
  rpc List(EvalResultListRequest) returns (EvalResultListResponse) {}
  rpc AssertionTable(AssertionTableRequest) returns (AssertionTableResponse) {}
  rpc GetEvalResult(GetEvalResultRequest) returns (GetEvalResultResponse) {}
}

message GetEvalResultRequest {
  // The ID of the example to fetch
  string id = 1;
}

message GetEvalResultResponse {
  string reportHTML = 1;
}
